{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this session, we will do some exercises about webscraping. Webscraping is the evil twin of API calls, seen last week. At the core of webscraping, there is a game: Usually, the website operators wants to prevent you from scraping her site as she would lose the ad revenues. It is therefore usually a fairly complex work where you have to outsmart somebody who makes part of her livelihood out of the data provided on the site.\n",
    "\n",
    "Moreover, scrapers are tipically much faster than humans and put therefore much more strain on the servers than the normal human user and require therefore more computing power on the part of the data provider. This long and boring introduction to say: please scrape responsibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nobel purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We know that the Nobel Prize in economics does not exist. The closest thing we have is the Nobel Memorial Prize in Economic Science. The first one was awarded in 1969, so it's fairly recent. This means that many of the work by the laureates are easily available online. In this week's exercises we will do two things:\n",
    "\n",
    "1. Scrape Wikipedia to retrieve informations about the location of the Alma Mater(s) of each Nobel Memorial Prize in Economic Sciences laureate. We want to know specifically how many Nobel Memorial Prize winner have never worked in an university in the United States.\n",
    "2. Retrieve quotes from those people in an automated fashion so that we can pick one at random in a \"quote of the day\" spirit.\n",
    "\n",
    "Wikipedia is a great source for this endeavour as the website curators are notably favorable towards scrapers and make little effort to try to deter them. It is therefore a great use case for a first foray into the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retrieving the name and wikipedia page of all the laureate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is a list of all the laureates available [here](https://en.wikipedia.org/wiki/List_of_Nobel_Memorial_Prize_laureates_in_Economics). The URL is \"https://en.wikipedia.org/wiki/List_of_Nobel_Memorial_Prize_laureates_in_Economics\". Let's just use requests and Beautiful Soup to retrieve all the names and wikipedia page of the laureates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that we have a list with the names and wikipedia pages of all laureates, we need to get inside each of those in order to get the name of their alma mater. The issue is that several of the laureates have several alma mater. Spend a little time looking at several of the pages of winners to see how this is reflected in the HTML.\n",
    "\n",
    "In this case, extracting the information might be wrapped inside a function that will be applied on the value of the dictionary created at the step before. Please use regular expression to identify the words \"Alma Mater\" (with and without capital letters for each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And now, repeat the same to identify in which country this university is located. No need to use regular expressions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now ready to put everything together. Put everything together to modify the structure in which you store the name of the laureates to contain also the countries of their alma mater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now ready to perform our analysis. Note however that what we have done could have been done in an more structured fashion had we used *pandas*, the library that enables working with dataframes.\n",
    "\n",
    "Moreover, you will see that some of the results are not correct and/or available (for example, Esther Duflo has no \"Alma Mater\" listed on her page (because in her case, it is called \"Education\") and some universities, such as the University of Leningrad, have no location). This is fine for now but a more comprehensive solution should determine something to work with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Voices from the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now going to collect all the quotes avaiable on Wikiquotes from the Nobel Memorial Prize for Economic Science laureates. In order to do so, we will pass through the list of laureates pages. If there is a link to a quotes page, we will scrape all the quotes there. If not, we will simply ignore it.\n",
    "\n",
    "When doing so, it usually makes sense to start by writing the code (in a function) that will scrape the quotes and then write the loop that pass on each laureate and identify if it is relevant to scrape it. Write the function that collects all the quotes from a wikiquote page. \n",
    "\n",
    "You will see that, for some authors, there are also quotes about the person (rather than quotes from the person). In this simple example, it is OK to collect them as well although in a real setting, we probably will want to avoid keeping those.\n",
    "\n",
    "If you need a page to serve as example, here is [Ragnar Frisch's wikiquotes page](https://en.wikiquote.org/wiki/Ragnar_Frisch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now going to write the part that goes through the pages of all laureates, checks whether they have a Wikiquote page (the link is in the left sidebar) and then retrieve the quotes. We can, as we did previously, put the quotes in a dictionary of lists. Try avoiding having lists of size 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that all the quotes are collected, we just use a standard random number generator to select one at random. You can either select a quote from a big \"bag\" containing all quotes but I would rather first choose a laureate and then a quote in order to avoid favouring the laureates who were more prolix (or who have a larger fan-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "Exercises.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
